import { HumanMessage } from '@langchain/core/messages';
import { Ollama } from '@langchain/ollama';

const main = async (): Promise<void> => {
  try {
    const llm = new Ollama({
      model: 'gpt-oss:20b',
      baseUrl: process.env.OLLAMA_BASE_URL || 'http://localhost:11434',
      temperature: 0.7,
      maxRetries: 2,
    });

    console.log('ðŸ¤– LangChain.js with Ollama');
    console.log('â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\n');

    const userPrompt: HumanMessage = new HumanMessage(
      'Explain what LangChain is in one sentence:'
    );
    console.log(`ðŸ“ Prompt: ${userPrompt.content}\n`);

    const response: string = await llm.invoke([userPrompt]);
    console.log(`ðŸ’¬ Response: ${response}\n`);
  } catch (error) {
    if (error instanceof Error) {
      console.error('âŒ Error:', error.message);
      if (
        error.message.includes('ECONNREFUSED') ||
        error.message.includes('fetch')
      ) {
        console.error(
          '\nâš ï¸  Make sure Ollama is running locally!\n' +
            '   Start it with: ollama serve\n' +
            "   Or ensure it's accessible at http://localhost:11434"
        );
      }
    } else {
      console.error('âŒ Unknown error occurred:', error);
    }
    process.exit(1);
  }
};

main().catch(error => {
  console.error('Unhandled error:', error);
  process.exit(1);
});
